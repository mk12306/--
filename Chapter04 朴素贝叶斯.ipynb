{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01简要介绍\n",
    "\n",
    "假设用p1(x, y)表示点(x, y)属于类别1的概率，p2(x, y)表示点(x, y)属于类别2的概率。  \n",
    "\n",
    "则，如果p1(x, y) > p2(x, y)，那么判定(x, y)属于类别1；  \n",
    "   如果p1(x, y) < p2(x, y)，那么判定(x, y)属于类别2；  \n",
    "   \n",
    "<font color=\"red\" size = 4>选择最高概率的决策，是贝叶斯决策理论的核心思想</font>\n",
    "\n",
    "![title.png](./picture/04朴素贝叶斯.png)\n",
    "\n",
    "__朴素贝叶斯的朴素指的是整个形式化过程只做最原始、最简单的假设。__\n",
    "\n",
    "<font color=\"red\" size = 4>·假设1：每个特征出现的可能性和其他特征无关，也即独立性；  \n",
    "·假设2：每个特征同等重要；</font>\n",
    "\n",
    "![title.png](./picture/04朴素贝叶斯的一般过程.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件概率 ：  p(A|B) = p(AB) / P(B)\n",
    "#### 贝叶斯准则：  \n",
    "![title.png](./picture/04贝叶斯准则.png)\n",
    "\n",
    "使用贝叶斯决策理论时，真正需要计算的是p(c1|(x, y)),p(c2|(x, y))。也即给定某个点(x, y)，计算该点分别属于c1,c2的概率。\n",
    "根据贝叶斯准则可以得到：\n",
    "![title.png](./picture/04贝叶斯.png)\n",
    "\n",
    "<font face=\"微软雅黑\"> <font color=\"red\" size = 6>Talk is cheap， show me the code.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.举例\n",
    "机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。\n",
    "虽然电子邮件是一种会不断增加的文本，但我们同样也可以对新闻报道、用户留言、政府公文等其他任意类型的文本进行分类。我们可以观察\n",
    "文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。朴素贝叶斯是用于文档分类的常用算法。\n",
    "\n",
    "假设词汇表中有1000个单词。要得到好的概率分布，就需要足够的数据样本，假定样本数为N。由统计学知，如果每个特征需要N个样本，\n",
    "那么对于10个特征将需要N^10个样本，对于包含1000个特征的词汇表将需要N^1000个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。\n",
    "\n",
    "根据朴素贝叶斯的独立性假设，如果特征之间相互独立，那么样本数就可以从N^1000减少到1000×N。\n",
    "\n",
    "要从文本中获取特征，需要先拆分文本。具体如何做呢？这里的特征是来自文本的词条（token），一个词条是字符的任意组合。可以把词条想象为单词，\n",
    "也可以使用非单词词条，如URL、IP地址或者任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文档中，0表示词条未出现。以在线社区的留言板为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类别：侮辱类和非侮辱类，使用1和0分别表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1准备数据，从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(): #简单创建样本和标签（是否包含侮辱性词汇）\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    \n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList, classVec\n",
    "\n",
    "def creatVocabList(dataSet): #创建一个包含所有文档中出现的不重复词列表\n",
    "    vocabSet = set([])     #创建空集\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  #两个集合的并集\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet): #输入词汇表和文档，判定文档中是否出现词汇表的单词\n",
    "    returnVec = [0] * len(vocabList)  #创建全为0的向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myVocabList: ['please', 'stop', 'food', 'dog', 'dalmation', 'so', 'has', 'garbage', 'him', 'is', 'ate', 'to', 'my', 'cute', 'maybe', 'help', 'problems', 'posting', 'licks', 'how', 'stupid', 'worthless', 'I', 'steak', 'quit', 'park', 'flea', 'love', 'mr', 'take', 'buying', 'not']\n",
      "listOPosts[0]: ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "myVec: [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet() #生成数据集\n",
    "myVocabList = creatVocabList(listOPosts) #创建词汇表\n",
    "print(\"myVocabList:\", myVocabList)\n",
    "\n",
    "print(\"listOPosts[0]:\", listOPosts[0])\n",
    "myVec = setOfWords2Vec(myVocabList, listOPosts[0]) #生成词向量，如果文档中有该词，置为1，否则置为0\n",
    "print(\"myVec:\", myVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2训练算法：从词向量计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入文档矩阵和文档类别标签构成的向量\n",
    "def trainNB0(trainmatrix, trainCategory):\n",
    "    numTrainDocs = len(trainmatrix)\n",
    "    numWords = len(trainmatrix[0])\n",
    "#     print(\"numWords:\", numWords)\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)  #计算侮辱性文档概率\n",
    "    p0Num = np.ones(numWords); p1Num = np.ones(numWords) #初始化概率为0,防止出现小概率溢出，初始化为1\n",
    "    p0Denom = 2.0; p1Denom = 2.0  #\n",
    "    for i in range(numTrainDocs): #计算p(wi | c1)和p(wi | c0)\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainmatrix[i]  #向量相加\n",
    "            p1Denom += sum(trainmatrix[i])\n",
    "        else:\n",
    "            p0Num += trainmatrix[i]\n",
    "            p0Denom += sum(trainmatrix[i])\n",
    "    p1Vect = p1Num / p1Denom    #对每个元素做除法\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pAb: 0.5\n",
      "p0V: [0.07692308 0.07692308 0.03846154 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.03846154 0.11538462 0.07692308 0.07692308 0.07692308\n",
      " 0.15384615 0.07692308 0.03846154 0.07692308 0.07692308 0.03846154\n",
      " 0.07692308 0.07692308 0.03846154 0.03846154 0.07692308 0.07692308\n",
      " 0.03846154 0.03846154 0.07692308 0.07692308 0.07692308 0.03846154\n",
      " 0.03846154 0.03846154]\n",
      "p1V: [0.04761905 0.0952381  0.0952381  0.14285714 0.04761905 0.04761905\n",
      " 0.04761905 0.0952381  0.0952381  0.04761905 0.04761905 0.0952381\n",
      " 0.04761905 0.04761905 0.0952381  0.04761905 0.04761905 0.0952381\n",
      " 0.04761905 0.04761905 0.19047619 0.14285714 0.04761905 0.04761905\n",
      " 0.0952381  0.0952381  0.04761905 0.04761905 0.04761905 0.0952381\n",
      " 0.0952381  0.0952381 ]\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = creatVocabList(listOPosts)\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "print(\"pAb:\", pAb)\n",
    "print(\"p0V:\", p0V)\n",
    "print(\"p1V:\", p1V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上面的结果发现p1V中概率最大的为下标26，0.19047619这个值，对应myVocabList中的stupid，这意味着stupid是最能表征类别1(侮辱性文档)的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + math.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + math.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = creatVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as :', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as:', classifyNB(thisDoc, p0V, p1V, pAb))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as : 0\n",
      "['stupid', 'garbage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实战一下\n",
    "\n",
    "__使用朴素贝叶斯过滤垃圾邮件__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
