{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01简要介绍\n",
    "\n",
    "假设用p1(x, y)表示点(x, y)属于类别1的概率，p2(x, y)表示点(x, y)属于类别2的概率。  \n",
    "\n",
    "则，如果p1(x, y) > p2(x, y)，那么判定(x, y)属于类别1；  \n",
    "   如果p1(x, y) < p2(x, y)，那么判定(x, y)属于类别2；  \n",
    "   \n",
    "<font color=\"red\" size = 4>选择最高概率的决策，是贝叶斯决策理论的核心思想</font>\n",
    "\n",
    "![title.png](./picture/04朴素贝叶斯.png)\n",
    "\n",
    "__朴素贝叶斯的朴素指的是整个形式化过程只做最原始、最简单的假设。__\n",
    "\n",
    "<font color=\"red\" size = 4>·假设1：每个特征出现的可能性和其他特征无关，也即独立性；  \n",
    "·假设2：每个特征同等重要；</font>\n",
    "\n",
    "![title.png](./picture/04朴素贝叶斯的一般过程.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件概率 ：  p(A|B) = p(AB) / P(B)\n",
    "#### 贝叶斯准则：  \n",
    "![title.png](./picture/04贝叶斯准则.png)\n",
    "\n",
    "使用贝叶斯决策理论时，真正需要计算的是p(c1|(x, y)),p(c2|(x, y))。也即给定某个点(x, y)，计算该点分别属于c1,c2的概率。\n",
    "根据贝叶斯准则可以得到：\n",
    "![title.png](./picture/04贝叶斯.png)\n",
    "\n",
    "<font face=\"微软雅黑\"> <font color=\"red\" size = 6>Talk is cheap， show me the code.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.举例\n",
    "机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。\n",
    "虽然电子邮件是一种会不断增加的文本，但我们同样也可以对新闻报道、用户留言、政府公文等其他任意类型的文本进行分类。我们可以观察\n",
    "文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。朴素贝叶斯是用于文档分类的常用算法。\n",
    "\n",
    "假设词汇表中有1000个单词。要得到好的概率分布，就需要足够的数据样本，假定样本数为N。由统计学知，如果每个特征需要N个样本，\n",
    "那么对于10个特征将需要N^10个样本，对于包含1000个特征的词汇表将需要N^1000个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。\n",
    "\n",
    "根据朴素贝叶斯的独立性假设，如果特征之间相互独立，那么样本数就可以从N^1000减少到1000×N。\n",
    "\n",
    "要从文本中获取特征，需要先拆分文本。具体如何做呢？这里的特征是来自文本的词条（token），一个词条是字符的任意组合。可以把词条想象为单词，\n",
    "也可以使用非单词词条，如URL、IP地址或者任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文档中，0表示词条未出现。以在线社区的留言板为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类别：侮辱类和非侮辱类，使用1和0分别表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1准备数据，从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(): #简单创建样本和标签（是否包含侮辱性词汇）\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    \n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList, classVec\n",
    "\n",
    "def creatVocabList(dataSet): #创建一个包含所有文档中出现的不重复词列表\n",
    "    vocabSet = set([])     #创建空集\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  #两个集合的并集\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet): #输入词汇表和文档，判定文档中是否出现词汇表的单词\n",
    "    returnVec = [0] * len(vocabList)  #创建全为0的向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myVocabList: ['dalmation', 'buying', 'ate', 'problems', 'him', 'love', 'garbage', 'mr', 'is', 'not', 'food', 'cute', 'I', 'has', 'stupid', 'licks', 'quit', 'my', 'maybe', 'park', 'posting', 'dog', 'stop', 'worthless', 'please', 'how', 'steak', 'take', 'flea', 'to', 'so', 'help']\n",
      "listOPosts[0]: ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "myVec: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet() #生成数据集\n",
    "myVocabList = creatVocabList(listOPosts) #创建词汇表\n",
    "print(\"myVocabList:\", myVocabList)\n",
    "\n",
    "print(\"listOPosts[0]:\", listOPosts[0])\n",
    "myVec = setOfWords2Vec(myVocabList, listOPosts[0]) #生成词向量，如果文档中有该词，置为1，否则置为0\n",
    "print(\"myVec:\", myVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2训练算法：从词向量计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入文档矩阵和文档类别标签构成的向量\n",
    "def trainNB0(trainmatrix, trainCategory):\n",
    "    numTrainDocs = len(trainmatrix)\n",
    "    numWords = len(trainmatrix[0])\n",
    "#     print(\"numWords:\", numWords)\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)  #计算侮辱性文档概率\n",
    "    p0Num = np.ones(numWords); p1Num = np.ones(numWords) #初始化概率为0,防止出现小概率溢出，初始化为1\n",
    "    p0Denom = 2.0; p1Denom = 2.0  #\n",
    "    for i in range(numTrainDocs): #计算p(wi | c1)和p(wi | c0)\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainmatrix[i]  #向量相加\n",
    "            p1Denom += sum(trainmatrix[i])\n",
    "        else:\n",
    "            p0Num += trainmatrix[i]\n",
    "            p0Denom += sum(trainmatrix[i])\n",
    "    p1Vect = p1Num / p1Denom    #对每个元素做除法\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pAb: 0.5\n",
      "p0V: [0.07692308 0.03846154 0.07692308 0.07692308 0.11538462 0.07692308\n",
      " 0.03846154 0.07692308 0.07692308 0.03846154 0.03846154 0.07692308\n",
      " 0.07692308 0.07692308 0.03846154 0.07692308 0.03846154 0.15384615\n",
      " 0.03846154 0.03846154 0.03846154 0.07692308 0.07692308 0.03846154\n",
      " 0.07692308 0.07692308 0.07692308 0.03846154 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308]\n",
      "p1V: [0.04761905 0.0952381  0.04761905 0.04761905 0.0952381  0.04761905\n",
      " 0.0952381  0.04761905 0.04761905 0.0952381  0.0952381  0.04761905\n",
      " 0.04761905 0.04761905 0.19047619 0.04761905 0.0952381  0.04761905\n",
      " 0.0952381  0.0952381  0.0952381  0.14285714 0.0952381  0.14285714\n",
      " 0.04761905 0.04761905 0.04761905 0.0952381  0.04761905 0.0952381\n",
      " 0.04761905 0.04761905]\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = creatVocabList(listOPosts)\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "print(\"pAb:\", pAb)\n",
    "print(\"p0V:\", p0V)\n",
    "print(\"p1V:\", p1V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上面的结果发现p1V中概率最大的为下标26，0.19047619这个值，对应myVocabList中的stupid，这意味着stupid是最能表征类别1(侮辱性文档)的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + math.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + math.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = creatVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as :', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as:', classifyNB(thisDoc, p0V, p1V, pAb))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as : 0\n",
      "['stupid', 'garbage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实战一下\n",
    "\n",
    "__使用朴素贝叶斯过滤垃圾邮件__\n",
    "\n",
    "![title.png](./picture/04朴素贝叶斯垃圾邮件分类.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysent = \"This book is the best book on Python or M.L Ihave ever laid eyes upon.\"\n",
    "print(mysent.split()) #string.split()函数能够自动切分字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'Ihave', 'ever', 'laid', 'eyes', 'upon', '']\n",
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'ihave', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "mysent = \"This book is the best book on Python or M.L Ihave ever laid eyes upon.\"\n",
    "regEx = re.compile('\\\\W')\n",
    "listOfTokens = regEx.split(mysent)\n",
    "print(listOfTokens)\n",
    "print([tok.lower() for tok in listOfTokens if len(tok) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '', '', 'Since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'Google', 'Groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', '', 'pages', 'or', 'files', '', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'February', '2011', '', 'We', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'Google', 'Groups', '', '', '', 'mailing', 'lists', 'and', 'forum', 'discussions', '', '', 'Instead', 'of', 'these', 'features', '', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', '', 'such', 'as', 'Google', 'Docs', 'and', 'Google', 'Sites', '', '', 'For', 'example', '', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'Google', 'Sites', 'and', 'share', 'the', 'site', '', 'http', '', '', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', '', 'with', 'the', 'members', 'of', 'your', 'group', '', 'You', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', '', 'http', '', '', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', '', 'on', 'the', 'site', '', 'If', 'you抮e', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', '', 'we', 'suggest', 'you', 'try', 'Google', 'Docs', '', 'You', 'can', 'upload', 'files', '', 'http', '', '', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', '', 'and', 'share', 'access', 'with', 'either', 'a', 'group', '', 'http', '', '', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', '', 'or', 'an', 'individual', '', 'http', '', '', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', '', '', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', '', '', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'Google', 'Groups', '']\n"
     ]
    }
   ],
   "source": [
    "emailText = open('./data/email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is: 0.3\n"
     ]
    }
   ],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\w', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    docList = []; classList = []; fullTest = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('./data/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('./data/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = creatVocabList(docList)\n",
    "    trainingSet = list(range(50)); testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    \n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print(\"the error rate is:\", float(errorCount) / len(testSet))   \n",
    "    \n",
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 函数spamTest()会输出在10封随机选择的电子邮件上的分类错误率。既然这些电子邮件是随机选择的，所以每次的输出结果可能有些差别。如果发现错误的话，函数会输出错分文档的词表，这样就可以了解到底是哪篇文档发生了错误。如果想要更好地估计错误率，那么就应该将上述过程重复多次，比如说10次，然后求平均值。我这么做了一下，获得的平均错误率为6%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
